{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f34eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5184f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vinai/phobert-base\"   # or \"xlm-roberta-base\"\n",
    "max_len = 128\n",
    "batch_size = 8\n",
    "lr = 2e-5\n",
    "num_epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "freeze_encoder = True     # True cho dataset nhỏ; False nếu muốn fine-tune encoder\n",
    "attn_num_heads = 4\n",
    "attn_dropout = 0.2\n",
    "save_path = \"best_bert_attn.pt\"\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55d5f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRCdataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128, include_response=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.texts = []\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        for _, r in df.iterrows():\n",
    "            context = str(r.get(\"context\",\"\"))\n",
    "            prompt  = str(r.get(\"prompt\",\"\"))\n",
    "            response = str(r.get(\"response\",\"\")) if include_response else \"\"\n",
    "            # build single string with simple separators\n",
    "            txt = f\"[CTX] {context} [Q] {prompt} [R] {response}\"\n",
    "            self.texts.append(txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: enc[k].squeeze(0) for k in enc}\n",
    "        item[\"label\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n",
    "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch])\n",
    "    labels = [b[\"label\"] for b in batch]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be1d5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention applied to encoder token embeddings.\n",
    "    Returns:\n",
    "      - attn_output: (B, L, H) (residual of attn)\n",
    "      - attn_weights: (B, num_heads, L, L)  (raw attention weights)\n",
    "    We'll pool weights over heads and take weights for CLS-like pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # hidden_states: (B, L, H)\n",
    "        # attention_mask: (B, L) with 1 for token, 0 for pad\n",
    "        # Build key_padding_mask for MultiheadAttention: True for positions to **ignore**\n",
    "        key_padding_mask = (attention_mask == 0)  # (B, L) bool\n",
    "        attn_out, attn_weights = self.mha(hidden_states, hidden_states, hidden_states,\n",
    "                                          key_padding_mask=key_padding_mask)  # attn_weights (B, L, L) or (B, num_heads, L, L) depending version\n",
    "        out = self.layernorm(hidden_states + self.dropout(attn_out))\n",
    "        # ensure attn_weights shape: (B, num_heads, L, L) if returned (B, L, L) convert to (B,1,L,L)\n",
    "        if attn_weights.dim() == 3:\n",
    "            attn_weights = attn_weights.unsqueeze(1)  # (B,1,L,L)\n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb612a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead=4, dim_feedforward=2048, nlayers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # x: (B,L,H). TransformerEncoder uses src_key_padding_mask where True = pad\n",
    "        src_key_padding_mask = (attention_mask == 0)\n",
    "        out = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        out = self.layernorm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7554ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttnLLMClassifier(nn.Module):\n",
    "    def __init__(self, encoder_name, num_classes, freeze_encoder=True,\n",
    "                 attn_heads=4, attn_dropout=0.1, small_enc_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        embed_dim = self.encoder.config.hidden_size\n",
    "        if freeze_encoder:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.attn_extractor = SelfAttentionExtractor(embed_dim, num_heads=attn_heads, dropout=attn_dropout)\n",
    "        # a small transformer acting as LLM-like encoder on top of attn output\n",
    "        self.small_encoder = SmallTransformerEncoder(d_model=embed_dim, nhead=min(attn_heads,8), nlayers=small_enc_layers)\n",
    "        # classification head: pool then MLP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embed_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, return_attn=False):\n",
    "        # Get Transformer encoder outputs (last_hidden_state)\n",
    "        enc_out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last_hidden = enc_out.last_hidden_state     # (B, L, H)\n",
    "\n",
    "        # Self-attention extractor (gives refined hidden + weights)\n",
    "        attn_out, attn_weights = self.attn_extractor(last_hidden, attention_mask)  # attn_out: (B,L,H)\n",
    "\n",
    "        # Small transformer encoder (LLM-like)\n",
    "        small_out = self.small_encoder(attn_out, attention_mask)  # (B,L,H)\n",
    "\n",
    "        # Pooling: mean pooled over valid tokens weighted by mask\n",
    "        mask = attention_mask.unsqueeze(-1)  # (B,L,1)\n",
    "        sum_hidden = (small_out * mask).sum(dim=1)               # (B,H)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = sum_hidden / denom                              # (B,H)\n",
    "\n",
    "        logits = self.classifier(pooled)                         # (B,num_classes)\n",
    "\n",
    "        if return_attn:\n",
    "            # produce per-token importance by averaging heads and taking attention from [all->token] or token->all\n",
    "            # attn_weights shape: (B, num_heads, L, L)\n",
    "            attn_avg = attn_weights.mean(dim=1)  # (B, L, L)\n",
    "            # For token importance we can take attention paid *to* each token: mean over source positions\n",
    "            token_imp = attn_avg.mean(dim=1)   # (B, L)\n",
    "            return logits, token_imp, attn_weights\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4793e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(train_csv, val_csv, test_csv, tokenizer, include_response=True):\n",
    "    df_train = pd.read_csv(train_csv).fillna(\"\")\n",
    "    df_val   = pd.read_csv(val_csv).fillna(\"\")\n",
    "    df_test  = pd.read_csv(test_csv).fillna(\"\")\n",
    "\n",
    "    # Label encoding consistently across sets\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(df_train[\"label\"].unique()) + list(df_val[\"label\"].unique()) + list(df_test[\"label\"].unique()))\n",
    "\n",
    "    # store string labels back to csv datasets (model input uses strings)\n",
    "    df_train[\"label\"] = le.transform(df_train[\"label\"])\n",
    "    df_val[\"label\"]   = le.transform(df_val[\"label\"])\n",
    "    df_test[\"label\"]  = le.transform(df_test[\"label\"])\n",
    "\n",
    "    train_ds = QRCdataset(df_train, tokenizer, max_len=max_len, include_response=include_response)\n",
    "    val_ds = QRCdataset(df_val, tokenizer, max_len=max_len, include_response=include_response)\n",
    "    test_ds = QRCdataset(df_test, tokenizer, max_len=max_len, include_response=include_response)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6cd4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = torch.tensor(batch[\"labels\"], dtype=torch.long).to(device) if isinstance(batch[\"labels\"], list) else batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids=input_ids, attention_mask=mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.argmax(logits, dim=-1).cpu().numpy().tolist())\n",
    "            trues.extend(labels.cpu().numpy().tolist())\n",
    "    avg_loss = np.mean(losses) if losses else 0.0\n",
    "    return avg_loss, preds, trues\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, scheduler, device, le):\n",
    "    best_f1 = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = torch.tensor(batch[\"labels\"], dtype=torch.long).to(device) if isinstance(batch[\"labels\"], list) else batch[\"labels\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids=input_ids, attention_mask=mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        val_loss, val_preds, val_trues = evaluate_model(model, val_loader, device)\n",
    "        prf = precision_recall_fscore_support(val_trues, val_preds, average=\"macro\", zero_division=0)\n",
    "        acc = accuracy_score(val_trues, val_preds)\n",
    "        print(f\"Epoch {epoch}: TrainLoss={train_loss:.4f} ValLoss={val_loss:.4f} Acc={acc:.4f} MacroF1={prf[2]:.4f}\")\n",
    "\n",
    "        if prf[2] > best_f1:\n",
    "            best_f1 = prf[2]\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"Saved best model, best macro-F1:\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77cb8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_attention(model, tokenizer, text, device, max_len=128):\n",
    "    model.eval()\n",
    "    enc = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    mask = enc[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits, token_imp, attn_weights = model(input_ids=input_ids, attention_mask=mask, return_attn=True)\n",
    "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        token_imp = token_imp.cpu().numpy()[0]  # (L,)\n",
    "        attn_weights = attn_weights.cpu().numpy()[0]  # (num_heads, L, L)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "    return probs, tokens, token_imp, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "791d69c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 18/18 [00:01<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: TrainLoss=1.1022 ValLoss=1.1052 Acc=0.3750 MacroF1=0.4042\n",
      "Saved best model, best macro-F1: 0.4041514041514041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 18/18 [00:01<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: TrainLoss=1.0784 ValLoss=1.1083 Acc=0.5000 MacroF1=0.4202\n",
      "Saved best model, best macro-F1: 0.42020202020202024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 18/18 [00:01<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: TrainLoss=1.0566 ValLoss=1.0948 Acc=0.4375 MacroF1=0.4481\n",
      "Saved best model, best macro-F1: 0.4481481481481482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 18/18 [00:01<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: TrainLoss=1.0382 ValLoss=1.0926 Acc=0.3750 MacroF1=0.3889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 18/18 [00:01<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: TrainLoss=1.0157 ValLoss=1.0934 Acc=0.4375 MacroF1=0.3796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 18/18 [00:01<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: TrainLoss=1.0051 ValLoss=1.0805 Acc=0.3750 MacroF1=0.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 18/18 [00:01<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: TrainLoss=0.9812 ValLoss=1.0774 Acc=0.3125 MacroF1=0.3296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 18/18 [00:01<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: TrainLoss=0.9574 ValLoss=1.0905 Acc=0.3750 MacroF1=0.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 18/18 [00:01<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: TrainLoss=0.9288 ValLoss=1.0787 Acc=0.5625 MacroF1=0.5305\n",
      "Saved best model, best macro-F1: 0.5304843304843305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 18/18 [00:01<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: TrainLoss=0.9054 ValLoss=1.0676 Acc=0.4375 MacroF1=0.3776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 18/18 [00:01<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: TrainLoss=0.8828 ValLoss=1.0631 Acc=0.4375 MacroF1=0.3776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 18/18 [00:01<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: TrainLoss=0.8581 ValLoss=1.0593 Acc=0.3750 MacroF1=0.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 18/18 [00:01<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: TrainLoss=0.8223 ValLoss=1.0484 Acc=0.3750 MacroF1=0.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 18/18 [00:01<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: TrainLoss=0.7867 ValLoss=1.0463 Acc=0.3750 MacroF1=0.3463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 18/18 [00:01<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: TrainLoss=0.7706 ValLoss=1.0426 Acc=0.4375 MacroF1=0.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 18/18 [00:01<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: TrainLoss=0.7369 ValLoss=1.0437 Acc=0.3750 MacroF1=0.3463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 18/18 [00:01<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: TrainLoss=0.7042 ValLoss=1.0356 Acc=0.4375 MacroF1=0.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 18/18 [00:01<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: TrainLoss=0.6753 ValLoss=1.0398 Acc=0.4375 MacroF1=0.4238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 18/18 [00:01<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: TrainLoss=0.6384 ValLoss=1.0325 Acc=0.5000 MacroF1=0.4940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 18/18 [00:01<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: TrainLoss=0.6152 ValLoss=1.0480 Acc=0.4375 MacroF1=0.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 18/18 [00:01<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: TrainLoss=0.5697 ValLoss=1.0541 Acc=0.5000 MacroF1=0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 18/18 [00:01<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: TrainLoss=0.5543 ValLoss=1.0710 Acc=0.3750 MacroF1=0.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 18/18 [00:01<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: TrainLoss=0.5075 ValLoss=1.0598 Acc=0.5000 MacroF1=0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 18/18 [00:01<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: TrainLoss=0.4956 ValLoss=1.0824 Acc=0.5000 MacroF1=0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 18/18 [00:01<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: TrainLoss=0.4492 ValLoss=1.0922 Acc=0.5000 MacroF1=0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 18/18 [00:01<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: TrainLoss=0.4406 ValLoss=1.1365 Acc=0.3750 MacroF1=0.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 18/18 [00:01<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: TrainLoss=0.4006 ValLoss=1.1124 Acc=0.5000 MacroF1=0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 18/18 [00:01<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: TrainLoss=0.3732 ValLoss=1.0963 Acc=0.5000 MacroF1=0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 18/18 [00:01<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: TrainLoss=0.3667 ValLoss=1.1511 Acc=0.4375 MacroF1=0.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 18/18 [00:01<00:00, 15.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: TrainLoss=0.3219 ValLoss=1.1052 Acc=0.5625 MacroF1=0.5607\n",
      "Saved best model, best macro-F1: 0.5606837606837607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 18/18 [00:01<00:00, 14.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: TrainLoss=0.2983 ValLoss=1.1568 Acc=0.4375 MacroF1=0.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 18/18 [00:01<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: TrainLoss=0.2560 ValLoss=1.1504 Acc=0.5000 MacroF1=0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 18/18 [00:01<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: TrainLoss=0.2719 ValLoss=1.1730 Acc=0.5625 MacroF1=0.5623\n",
      "Saved best model, best macro-F1: 0.5622710622710623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 18/18 [00:01<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: TrainLoss=0.2373 ValLoss=1.2324 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 18/18 [00:01<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: TrainLoss=0.2328 ValLoss=1.1709 Acc=0.5625 MacroF1=0.5607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 18/18 [00:01<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: TrainLoss=0.1897 ValLoss=1.3031 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 18/18 [00:01<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: TrainLoss=0.1981 ValLoss=1.2159 Acc=0.5000 MacroF1=0.4905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 18/18 [00:01<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: TrainLoss=0.1559 ValLoss=1.2845 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 18/18 [00:01<00:00, 14.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: TrainLoss=0.1624 ValLoss=1.3196 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 18/18 [00:01<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: TrainLoss=0.1623 ValLoss=1.3512 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 18/18 [00:01<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: TrainLoss=0.1366 ValLoss=1.3012 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 18/18 [00:01<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: TrainLoss=0.1234 ValLoss=1.3299 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 18/18 [00:01<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: TrainLoss=0.1344 ValLoss=1.3595 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 18/18 [00:01<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: TrainLoss=0.1330 ValLoss=1.4305 Acc=0.4375 MacroF1=0.4312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 18/18 [00:01<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: TrainLoss=0.1167 ValLoss=1.3088 Acc=0.5625 MacroF1=0.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 18/18 [00:01<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: TrainLoss=0.1153 ValLoss=1.3247 Acc=0.5625 MacroF1=0.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 18/18 [00:01<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: TrainLoss=0.1045 ValLoss=1.3863 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 18/18 [00:01<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: TrainLoss=0.0950 ValLoss=1.3827 Acc=0.5000 MacroF1=0.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 18/18 [00:01<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: TrainLoss=0.0847 ValLoss=1.3862 Acc=0.5000 MacroF1=0.5022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 18/18 [00:01<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: TrainLoss=0.0988 ValLoss=1.4549 Acc=0.5000 MacroF1=0.5016\n",
      "=== Test Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   extrinsic       0.50      0.69      0.58        13\n",
      "   intrinsic       0.62      0.62      0.62        13\n",
      "          no       0.44      0.29      0.35        14\n",
      "\n",
      "    accuracy                           0.53        40\n",
      "   macro avg       0.52      0.53      0.51        40\n",
      "weighted avg       0.52      0.53      0.51        40\n",
      "\n",
      "Pred probs: [0.82819843 0.15018632 0.02161524]\n",
      "8 chuyển 0.010460099205374718\n",
      "9 tiếp 0.009309806860983372\n",
      "16 đồng 0.008942298591136932\n",
      "75 mét 0.008940604515373707\n",
      "111 mé@@ 0.008901253342628479\n",
      "32 và 0.008878671564161777\n",
      "17 bằng 0.008838163688778877\n",
      "93 - 0.008818496949970722\n",
      "30 xuống 0.008793191984295845\n",
      "118 mé@@ 0.008790582418441772\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # paths\n",
    "    train_csv = \"train.csv\"\n",
    "    val_csv = \"val.csv\"\n",
    "    test_csv = \"test.csv\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    train_ds, val_ds, test_ds, le = prepare_datasets(train_csv, val_csv, test_csv, tokenizer, include_response=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    num_classes = len(le.classes_)\n",
    "    model = BertAttnLLMClassifier(model_name, num_classes=num_classes, freeze_encoder=freeze_encoder,\n",
    "                                  attn_heads=attn_num_heads, attn_dropout=attn_dropout, small_enc_layers=1).to(device)\n",
    "\n",
    "    # optimizer + scheduler\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-2)\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=total_steps)\n",
    "\n",
    "    # train\n",
    "    train_loop(train_loader, val_loader, model, optimizer, scheduler, device, le)\n",
    "\n",
    "    # load best and evaluate on test\n",
    "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "    test_loss, test_preds, test_trues = evaluate_model(model, test_loader, device)\n",
    "    print(\"=== Test Report ===\")\n",
    "    print(classification_report(test_trues, test_preds, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "    # example inference with attention\n",
    "    sample_text = pd.read_csv(test_csv).iloc[0]\n",
    "    text = f\"[CTX] {sample_text['context']} [Q] {sample_text['prompt']} [R] {sample_text['response']}\"\n",
    "    probs, tokens, token_imp, attn_weights = predict_with_attention(model, tokenizer, text, device, max_len=max_len)\n",
    "    print(\"Pred probs:\", probs)\n",
    "    # show top-k tokens by importance\n",
    "    topk = np.argsort(token_imp)[-10:][::-1]\n",
    "    for idx in topk:\n",
    "        print(idx, tokens[idx], float(token_imp[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208ca821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0029139161109923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   extrinsic       0.50      0.31      0.38        13\n",
      "   intrinsic       0.50      0.69      0.58        13\n",
      "          no       0.50      0.50      0.50        14\n",
      "\n",
      "    accuracy                           0.50        40\n",
      "   macro avg       0.50      0.50      0.49        40\n",
      "weighted avg       0.50      0.50      0.49        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- Evaluate on test set + print per-class report ----------\n",
    "model.load_state_dict(torch.load(\"best_model_attn.pt\"))\n",
    "test_loss, test_preds, test_trues = evaluate_model(model, test_loader, device)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(classification_report(test_trues, test_preds, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1e519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 đồng 0.008845827\n",
      "5 tiếp 0.0087670665\n",
      "13 bằng 0.008642147\n",
      "18 địa 0.008587842\n",
      "110 nơi 0.008553644\n",
      "122 , 0.0084927995\n",
      "4 chuyển 0.008469682\n",
      "51 bình 0.008415734\n",
      "19 hình 0.008396566\n",
      "118 vực 0.00839231\n",
      "Pred probs: [0.33478364 0.46328855 0.2019278 ]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Inference example: trả về attention scores để giải thích ----------\n",
    "def predict_with_attention(model, tokenizer, text, device, max_len=128):\n",
    "    model.eval()\n",
    "    enc = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    mask = enc[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits, attn_weights = model(input_ids=input_ids, attention_mask=mask, return_attn=True)\n",
    "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        weights = attn_weights.cpu().numpy()[0]  # (seq_len,)\n",
    "    # map tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "    return probs, tokens, weights\n",
    "\n",
    "# ví dụ:\n",
    "text = df_test.iloc[0][\"context\"] + \" \" + df_test.iloc[0][\"prompt\"] + \" \" + df_test.iloc[0][\"response\"]\n",
    "probs, tokens, weights = predict_with_attention(model, tokenizer, text, device)\n",
    "# show top tokens with high attention\n",
    "topk = np.argsort(weights)[-10:][::-1]\n",
    "for idx in topk:\n",
    "    print(idx, tokens[idx], weights[idx])\n",
    "print(\"Pred probs:\", probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
