{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65f993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\acer\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 8.4/18.2 MB 40.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.8/18.2 MB 44.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 41.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 46.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 46.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "   ---------------------------------------- 0.0/561.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 561.5/561.5 kB ? eta 0:00:00\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 72.4 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.2\n",
      "    Uninstalling huggingface-hub-0.33.2:\n",
      "      Successfully uninstalled huggingface-hub-0.33.2\n",
      "Successfully installed faiss-cpu-1.12.0 huggingface-hub-0.34.4 sentence-transformers-5.1.0 tokenizers-0.21.4 transformers-4.55.4\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836e1d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3383322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 13.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd135b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\acer\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "765f737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay đường dẫn dataset của bạn\n",
    "dataset_path = \"vihallu-warmup.csv\"\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3120879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences_vi(text):\n",
    "    \"\"\"\n",
    "    Tách câu tiếng Việt bằng regex.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', str(text).strip())\n",
    "    return [s for s in sentences if s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8700a354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1123 sentences from context.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"keepitreal/vietnamese-sbert\")  # pretrained cho tiếng Việt\n",
    "\n",
    "all_chunks = []\n",
    "chunk_map = []  # lưu mapping chunk -> index\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    context = row[\"context\"]\n",
    "    sentences = split_sentences_vi(context)\n",
    "    for sent in sentences:\n",
    "        all_chunks.append(sent)\n",
    "        chunk_map.append(i)\n",
    "\n",
    "# Embed tất cả context chunks\n",
    "context_embeddings = model.encode(all_chunks, convert_to_numpy=True)\n",
    "\n",
    "# Build FAISS index\n",
    "d = context_embeddings.shape[1]  # dimension\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(context_embeddings)\n",
    "\n",
    "print(f\"Indexed {len(all_chunks)} sentences from context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3689c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  \\\n",
      "0  c480e0d2-72e5-47d6-baca-a884db935c8c   \n",
      "1  8e225849-4ed1-4397-8519-2b4cfee0c7c6   \n",
      "2  c5a2aef8-4c3f-4bac-9fc3-814094578fc0   \n",
      "3  0bae8fee-cd84-4ec1-a324-0777f6fa7a32   \n",
      "4  a8dcd1ed-c9a1-4786-b97b-27244896ff95   \n",
      "\n",
      "                                             context  \\\n",
      "0  Theo pháp lệnh Vincennes năm 1374, vương quốc ...   \n",
      "1  Hệ thống đường biển xuất phát từ các cảng biển...   \n",
      "2  Năm 1928, Bộ Giao thông khởi thảo kế hoạch côn...   \n",
      "3  Sự tiến hóa của giới thực vật đã theo xu hướng...   \n",
      "4  Kể từ những năm 1970, chính phủ cũng thực hiện...   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  Quyền phản đối được khôi phục bởi Orléans đã g...   \n",
      "1  Những sông lớn nào là các tuyến đường thủy nội...   \n",
      "2  Độ dài công lộ Hán Trung-Thất Bàn Quan là bao ...   \n",
      "3  Vì thực vật hạt trần chiếm ưu thế trong việc t...   \n",
      "4  Cacs loaji dược pham muôn vào thi trường My ph...   \n",
      "\n",
      "                                            response      label  \n",
      "0  Quyền phản đối còn cho phép Nghị viện được bổ ...  extrinsic  \n",
      "1  Các tuyến đường thủy nội địa huyết mạch chạy t...  intrinsic  \n",
      "2  Công lộ Hán Trung-Thất Bàn Quan dài hơn 150 km...  extrinsic  \n",
      "3  Thực vật hạt trần kém đa dạng hơn và hiếm gặp ...         no  \n",
      "4  Các loại dược phẩm muốn vào thị trường Mỹ phải...         no  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb917890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top retrieved context:\n",
      "- Để đổi lấy sự ủng hộ của họ, Orléans cho khôi phục droit de remontrance (quyền phản đối) của Nghị viện - vốn bị Louis XIV triệt bỏ từ trước, theo đó Nghị viện có quyền phản đối những quyết định của nhà vua mà họ cho là trái với lợi ích dân tộc.\n",
      "- Trong nhiều năm, người Mỹ gốc châu Phi phải đối mặt với bạo động chống lại họ nhưng họ đạt được những bước vĩ đại về công bằng xã hội qua các phán quyết của tối cao pháp viện trong đó có các vụ án như Brown đối đầu Ban Giáo dục và Loving đối đầu Virginia, Đạo luật Dân quyền 1964, Đạo luật Quyền đầu phiếu 1965, và Đạo luật Nhà ở Công bằng 1968 mà qua đó kết thúc luật Jim Crow từng hợp thức hóa tách ly chủng tộc giữa người da trắng và người da đen.\n",
      "- Tên chính thức của lãnh thổ Nouvelle-Calédonie có thể được thay đổi trong tương lai gần theo hiệp nghị, theo đó \"một danh xưng, một hiệu kỳ, một bài ca, một khẩu hiệu và một thiết kế tiền giấy sẽ do toàn thể các đảng phải cùng nhau tìm kiếm, nhằm thể hiện bản sắc Kanak và tương lai do toàn bộ các đảng phái chia sẻ.\" Tuy nhiên, cho đến nay không có đồng thuận về tên gọi mới cho lãnh thổ.\n"
     ]
    }
   ],
   "source": [
    "query = \"  Quyền phản đối được khôi phục bởi?\"\n",
    "query_emb = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "k = 3\n",
    "D, I = index.search(query_emb, k)\n",
    "print(\"Top retrieved context:\")\n",
    "for idx in I[0]:\n",
    "    print(\"-\", all_chunks[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66bae230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (198, 768)\n",
      "Labels: ['extrinsic' 'intrinsic' 'no']\n"
     ]
    }
   ],
   "source": [
    "X_embeddings = []\n",
    "y = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Embed prompt + response\n",
    "    pr_emb = model.encode(\n",
    "        row[\"prompt\"] + \" \" + row[\"response\"], \n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    X_embeddings.append(pr_emb)\n",
    "    y.append(row[\"label\"])\n",
    "\n",
    "X = np.array(X_embeddings)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Labels:\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf18412",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f97859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "563d8ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước ban đầu: (198, 5)\n",
      "                                     id  \\\n",
      "0  c480e0d2-72e5-47d6-baca-a884db935c8c   \n",
      "1  8e225849-4ed1-4397-8519-2b4cfee0c7c6   \n",
      "2  c5a2aef8-4c3f-4bac-9fc3-814094578fc0   \n",
      "3  0bae8fee-cd84-4ec1-a324-0777f6fa7a32   \n",
      "4  a8dcd1ed-c9a1-4786-b97b-27244896ff95   \n",
      "\n",
      "                                             context  \\\n",
      "0  Theo pháp lệnh Vincennes năm 1374, vương quốc ...   \n",
      "1  Hệ thống đường biển xuất phát từ các cảng biển...   \n",
      "2  Năm 1928, Bộ Giao thông khởi thảo kế hoạch côn...   \n",
      "3  Sự tiến hóa của giới thực vật đã theo xu hướng...   \n",
      "4  Kể từ những năm 1970, chính phủ cũng thực hiện...   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  Quyền phản đối được khôi phục bởi Orléans đã g...   \n",
      "1  Những sông lớn nào là các tuyến đường thủy nội...   \n",
      "2  Độ dài công lộ Hán Trung-Thất Bàn Quan là bao ...   \n",
      "3  Vì thực vật hạt trần chiếm ưu thế trong việc t...   \n",
      "4  Cacs loaji dược pham muôn vào thi trường My ph...   \n",
      "\n",
      "                                            response      label  \n",
      "0  Quyền phản đối còn cho phép Nghị viện được bổ ...  extrinsic  \n",
      "1  Các tuyến đường thủy nội địa huyết mạch chạy t...  intrinsic  \n",
      "2  Công lộ Hán Trung-Thất Bàn Quan dài hơn 150 km...  extrinsic  \n",
      "3  Thực vật hạt trần kém đa dạng hơn và hiếm gặp ...         no  \n",
      "4  Các loại dược phẩm muốn vào thị trường Mỹ phải...         no  \n"
     ]
    }
   ],
   "source": [
    "print(\"Kích thước ban đầu:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0b681bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d42de28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Theo pháp lệnh Vincennes năm 1374, vương quốc ...</td>\n",
       "      <td>Quyền phản đối được khôi phục bởi Orléans đã g...</td>\n",
       "      <td>Quyền phản đối còn cho phép Nghị viện được bổ ...</td>\n",
       "      <td>extrinsic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hệ thống đường biển xuất phát từ các cảng biển...</td>\n",
       "      <td>Những sông lớn nào là các tuyến đường thủy nội...</td>\n",
       "      <td>Các tuyến đường thủy nội địa huyết mạch chạy t...</td>\n",
       "      <td>intrinsic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Năm 1928, Bộ Giao thông khởi thảo kế hoạch côn...</td>\n",
       "      <td>Độ dài công lộ Hán Trung-Thất Bàn Quan là bao ...</td>\n",
       "      <td>Công lộ Hán Trung-Thất Bàn Quan dài hơn 150 km...</td>\n",
       "      <td>extrinsic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sự tiến hóa của giới thực vật đã theo xu hướng...</td>\n",
       "      <td>Vì thực vật hạt trần chiếm ưu thế trong việc t...</td>\n",
       "      <td>Thực vật hạt trần kém đa dạng hơn và hiếm gặp ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kể từ những năm 1970, chính phủ cũng thực hiện...</td>\n",
       "      <td>Cacs loaji dược pham muôn vào thi trường My ph...</td>\n",
       "      <td>Các loại dược phẩm muốn vào thị trường Mỹ phải...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Theo pháp lệnh Vincennes năm 1374, vương quốc ...   \n",
       "1  Hệ thống đường biển xuất phát từ các cảng biển...   \n",
       "2  Năm 1928, Bộ Giao thông khởi thảo kế hoạch côn...   \n",
       "3  Sự tiến hóa của giới thực vật đã theo xu hướng...   \n",
       "4  Kể từ những năm 1970, chính phủ cũng thực hiện...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Quyền phản đối được khôi phục bởi Orléans đã g...   \n",
       "1  Những sông lớn nào là các tuyến đường thủy nội...   \n",
       "2  Độ dài công lộ Hán Trung-Thất Bàn Quan là bao ...   \n",
       "3  Vì thực vật hạt trần chiếm ưu thế trong việc t...   \n",
       "4  Cacs loaji dược pham muôn vào thi trường My ph...   \n",
       "\n",
       "                                            response      label  \n",
       "0  Quyền phản đối còn cho phép Nghị viện được bổ ...  extrinsic  \n",
       "1  Các tuyến đường thủy nội địa huyết mạch chạy t...  intrinsic  \n",
       "2  Công lộ Hán Trung-Thất Bàn Quan dài hơn 150 km...  extrinsic  \n",
       "3  Thực vật hạt trần kém đa dạng hơn và hiếm gặp ...         no  \n",
       "4  Các loại dược phẩm muốn vào thị trường Mỹ phải...         no  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "936d21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số dòng chứa NaN: context     0\n",
      "prompt      0\n",
      "response    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Số dòng chứa NaN:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3674669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"context\"] = df[\"context\"].astype(str).str.strip()\n",
    "df[\"prompt\"]  = df[\"prompt\"].astype(str).str.strip()\n",
    "df[\"response\"] = df[\"response\"].astype(str).str.strip()\n",
    "df[\"label\"]   = df[\"label\"].astype(str).str.strip().str.lower()   # giả sử label là số"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a7c5fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các nhãn duy nhất: ['extrinsic' 'intrinsic' 'no']\n",
      "Phân bố nhãn:\n",
      "label\n",
      "extrinsic    66\n",
      "intrinsic    66\n",
      "no           66\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Các nhãn duy nhất:\", df[\"label\"].unique())\n",
    "print(\"Phân bố nhãn:\")\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef6eba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (142, 4)\n",
      "Validation: (16, 4)\n",
      "Test: (40, 4)\n"
     ]
    }
   ],
   "source": [
    "# 1.5. Tách tập train/val/test\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    stratify=df[\"label\"], \n",
    "    random_state=42\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.1, \n",
    "    stratify=train_df[\"label\"], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Validation:\", val_df.shape)\n",
    "print(\"Test:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23301455",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8f09e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ví dụ sau khi tiền xử lý:\n",
      "                                              response  \\\n",
      "188  Ba tổ chức Việt Minh, Việt Quốc, Việt Cách thự...   \n",
      "83   Ngoài Matthias Sammer và Ulf Kirsten, một số c...   \n",
      "63   Các bằng chứng về lịch sử Trái Đất tại Phong N...   \n",
      "85   Năm 161, hoàng đế La Mã là Traianus, người đã ...   \n",
      "174  Sông Hồng và sông Đà là các tuyến đường thủy n...   \n",
      "9    Kiến chúa là con kiến duy nhất chịu trách nhiệ...   \n",
      "104  Vua Hán đã quyết định giải tán toàn bộ quân độ...   \n",
      "18   Giới tính của Richard đã trở thành mối quan tâ...   \n",
      "100  Cuộc họp của đảng Bolshevik không nhằm ủng hộ ...   \n",
      "42   Sự xuất hiện của máy tính cá nhân không ảnh hư...   \n",
      "\n",
      "                                        response_clean  \n",
      "188  ba tổ chức việt minh việt quốc việt cách thực ...  \n",
      "83   ngoài matthias sammer và ulf kirsten một số cầ...  \n",
      "63   các bằng chứng về lịch sử trái đất tại phong n...  \n",
      "85   năm 161 hoàng đế la mã là traianus người đã dẫ...  \n",
      "174  sông hồng và sông đà là các tuyến đường thủy n...  \n",
      "9    kiến chúa là con kiến duy nhất chịu trách nhiệ...  \n",
      "104  vua hán đã quyết định giải tán toàn bộ quân độ...  \n",
      "18   giới tính của richard đã trở thành mối quan tâ...  \n",
      "100  cuộc họp của đảng bolshevik không nhằm ủng hộ ...  \n",
      "42   sự xuất hiện của máy tính cá nhân không ảnh hư...  \n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# BƯỚC 2. Tiền xử lý văn bản (không segmentation)\n",
    "# ============================\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()  # về chữ thường\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)  # bỏ link\n",
    "    text = re.sub(r\"[^0-9a-zA-ZÀ-ỹ\\s]\", \" \", text)  # bỏ ký tự đặc biệt\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # xóa khoảng trắng thừa\n",
    "    return text\n",
    "\n",
    "# Áp dụng cho train/val/test\n",
    "for split, df_split in zip(\n",
    "    [\"train\", \"val\", \"test\"], \n",
    "    [train_df, val_df, test_df]\n",
    "):\n",
    "    df_split[\"context_clean\"]  = df_split[\"context\"].apply(clean_text)\n",
    "    df_split[\"prompt_clean\"]   = df_split[\"prompt\"].apply(clean_text)\n",
    "    df_split[\"response_clean\"] = df_split[\"response\"].apply(clean_text)\n",
    "\n",
    "    # Lưu lại file đã tiền xử lý\n",
    "    df_split.to_csv(f\"{split}_clean.csv\", index=False)\n",
    "\n",
    "print(\"Ví dụ sau khi tiền xử lý:\")\n",
    "print(train_df[[\"response\", \"response_clean\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc79fe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d640eb4b0cd24b7baf74f0380d3381cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a588e42b64940aca8d6d4e945f1040e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228e5c6a3ed94ca5addafb36650a0e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (142, 384)\n",
      "Ví dụ embedding 1 sample: [ 0.01581215  0.34333    -0.05604008 -0.03048981  0.11295061  0.17085275\n",
      " -0.06542513 -0.06371505  0.13694264  0.1768651 ]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# BƯỚC 3. Tạo Embedding\n",
    "# ============================\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load model đa ngôn ngữ (hỗ trợ tiếng Việt tốt)\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def make_input(row):\n",
    "    return f\"context: {row['context_clean']} [SEP] prompt: {row['prompt_clean']} [SEP] response: {row['response_clean']}\"\n",
    "\n",
    "# Hàm tạo embedding cho một dataframe\n",
    "def encode_dataframe(df, model):\n",
    "    texts = df.apply(make_input, axis=1).tolist()\n",
    "    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Tạo embedding cho train/val/test\n",
    "X_train = encode_dataframe(train_df, model)\n",
    "X_val   = encode_dataframe(val_df, model)\n",
    "X_test  = encode_dataframe(test_df, model)\n",
    "\n",
    "# Lưu label (string -> giữ nguyên để xử lý sau)\n",
    "y_train = train_df[\"label\"].values\n",
    "y_val   = val_df[\"label\"].values\n",
    "y_test  = test_df[\"label\"].values\n",
    "\n",
    "# Lưu ra file npy để dùng sau\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Ví dụ embedding 1 sample:\", X_train[0][:10])  # in 10 giá trị đầu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "743432ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=128, num_heads=4, num_layers=2):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "\n",
    "        # Biến đổi input embedding thành sequence (giả sử 384 → 12 tokens, mỗi token 32-d)\n",
    "        self.seq_len = 12\n",
    "        self.token_dim = input_dim // self.seq_len  # 384/12 = 32\n",
    "        self.proj = nn.Linear(input_dim, self.seq_len * self.token_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.token_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.3,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(self.token_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, input_dim)\n",
    "        x = self.proj(x)                         # (batch, seq_len*token_dim)\n",
    "        x = x.view(-1, self.seq_len, self.token_dim)  # (batch, seq_len, token_dim)\n",
    "\n",
    "        # Transformer encoder\n",
    "        x = self.transformer(x)   # (batch, seq_len, token_dim)\n",
    "\n",
    "        # Pooling (lấy mean của các token)\n",
    "        x = x.mean(dim=1)         # (batch, token_dim)\n",
    "\n",
    "        # Classification\n",
    "        out = self.fc(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cff86238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Epoch 1, Loss=9.8712, Val Acc=0.3125\n",
      "Epoch 2, Loss=9.4921, Val Acc=0.5000\n",
      "Epoch 3, Loss=9.0809, Val Acc=0.3750\n",
      "Epoch 4, Loss=8.7036, Val Acc=0.3750\n",
      "Epoch 5, Loss=8.2500, Val Acc=0.5625\n",
      "Epoch 6, Loss=7.7135, Val Acc=0.5000\n",
      "Epoch 7, Loss=7.2980, Val Acc=0.5000\n",
      "Epoch 8, Loss=6.6060, Val Acc=0.6250\n",
      "Epoch 9, Loss=6.1505, Val Acc=0.6250\n",
      "Epoch 10, Loss=5.6918, Val Acc=0.6250\n",
      "Epoch 11, Loss=5.2887, Val Acc=0.6250\n",
      "Epoch 12, Loss=4.8855, Val Acc=0.5625\n",
      "Epoch 13, Loss=4.4552, Val Acc=0.6250\n",
      "Epoch 14, Loss=3.9488, Val Acc=0.5625\n",
      "Epoch 15, Loss=3.7368, Val Acc=0.5625\n",
      "Epoch 16, Loss=3.3647, Val Acc=0.5625\n",
      "Epoch 17, Loss=2.9746, Val Acc=0.5625\n",
      "Epoch 18, Loss=2.7572, Val Acc=0.6250\n",
      "Epoch 19, Loss=2.4858, Val Acc=0.5625\n",
      "Epoch 20, Loss=2.4000, Val Acc=0.6250\n",
      "Epoch 21, Loss=2.0085, Val Acc=0.5625\n",
      "Epoch 22, Loss=1.9529, Val Acc=0.6250\n",
      "Epoch 23, Loss=1.5743, Val Acc=0.6250\n",
      "Epoch 24, Loss=1.5187, Val Acc=0.6250\n",
      "Epoch 25, Loss=1.4120, Val Acc=0.6250\n",
      "Epoch 26, Loss=1.3999, Val Acc=0.6250\n",
      "Epoch 27, Loss=1.2157, Val Acc=0.6250\n",
      "Epoch 28, Loss=1.0954, Val Acc=0.6250\n",
      "Epoch 29, Loss=0.9655, Val Acc=0.6250\n",
      "Epoch 30, Loss=0.8981, Val Acc=0.6250\n",
      "Epoch 31, Loss=0.8690, Val Acc=0.6250\n",
      "Epoch 32, Loss=0.8571, Val Acc=0.6250\n",
      "Epoch 33, Loss=0.7876, Val Acc=0.6250\n",
      "Epoch 34, Loss=0.7497, Val Acc=0.6250\n",
      "Epoch 35, Loss=0.6478, Val Acc=0.6250\n",
      "Epoch 36, Loss=0.6096, Val Acc=0.6250\n",
      "Epoch 37, Loss=0.5168, Val Acc=0.5625\n",
      "Epoch 38, Loss=0.5162, Val Acc=0.6250\n",
      "Epoch 39, Loss=0.5225, Val Acc=0.6250\n",
      "Epoch 40, Loss=0.4408, Val Acc=0.5625\n",
      "Epoch 41, Loss=0.4588, Val Acc=0.5625\n",
      "Epoch 42, Loss=0.3841, Val Acc=0.5625\n",
      "Epoch 43, Loss=0.3919, Val Acc=0.5625\n",
      "Epoch 44, Loss=0.2875, Val Acc=0.5625\n",
      "Epoch 45, Loss=0.3303, Val Acc=0.5625\n",
      "Epoch 46, Loss=0.3052, Val Acc=0.5625\n",
      "Epoch 47, Loss=0.3305, Val Acc=0.5625\n",
      "Epoch 48, Loss=0.3177, Val Acc=0.5625\n",
      "Epoch 49, Loss=0.2399, Val Acc=0.5625\n",
      "Epoch 50, Loss=0.2603, Val Acc=0.5625\n",
      "Epoch 51, Loss=0.2536, Val Acc=0.5625\n",
      "Epoch 52, Loss=0.2304, Val Acc=0.5625\n",
      "Epoch 53, Loss=0.2898, Val Acc=0.5625\n",
      "Epoch 54, Loss=0.2387, Val Acc=0.5625\n",
      "Epoch 55, Loss=0.2461, Val Acc=0.5625\n",
      "Epoch 56, Loss=0.1809, Val Acc=0.5625\n",
      "Epoch 57, Loss=0.1995, Val Acc=0.5625\n",
      "Epoch 58, Loss=0.2220, Val Acc=0.5625\n",
      "Epoch 59, Loss=0.1900, Val Acc=0.5625\n",
      "Epoch 60, Loss=0.1902, Val Acc=0.5625\n",
      "Epoch 61, Loss=0.1653, Val Acc=0.5625\n",
      "Epoch 62, Loss=0.1709, Val Acc=0.5625\n",
      "Epoch 63, Loss=0.1615, Val Acc=0.5625\n",
      "Epoch 64, Loss=0.1703, Val Acc=0.5625\n",
      "Epoch 65, Loss=0.1545, Val Acc=0.5625\n",
      "Epoch 66, Loss=0.1411, Val Acc=0.5625\n",
      "Epoch 67, Loss=0.1348, Val Acc=0.5625\n",
      "Epoch 68, Loss=0.1330, Val Acc=0.5625\n",
      "Epoch 69, Loss=0.1280, Val Acc=0.5625\n",
      "Epoch 70, Loss=0.1227, Val Acc=0.5625\n",
      "Epoch 71, Loss=0.1275, Val Acc=0.5625\n",
      "Epoch 72, Loss=0.1165, Val Acc=0.5625\n",
      "Epoch 73, Loss=0.0926, Val Acc=0.5625\n",
      "Epoch 74, Loss=0.1045, Val Acc=0.5625\n",
      "Epoch 75, Loss=0.1137, Val Acc=0.5625\n",
      "Epoch 76, Loss=0.1096, Val Acc=0.5625\n",
      "Epoch 77, Loss=0.0912, Val Acc=0.5625\n",
      "Epoch 78, Loss=0.0946, Val Acc=0.5625\n",
      "Epoch 79, Loss=0.0914, Val Acc=0.5625\n",
      "Epoch 80, Loss=0.1017, Val Acc=0.5625\n",
      "Epoch 81, Loss=0.0938, Val Acc=0.5625\n",
      "Epoch 82, Loss=0.0864, Val Acc=0.5625\n",
      "Epoch 83, Loss=0.1001, Val Acc=0.5625\n",
      "Epoch 84, Loss=0.0878, Val Acc=0.5625\n",
      "Epoch 85, Loss=0.0756, Val Acc=0.5625\n",
      "Epoch 86, Loss=0.0699, Val Acc=0.5625\n",
      "Epoch 87, Loss=0.0734, Val Acc=0.5625\n",
      "Epoch 88, Loss=0.0773, Val Acc=0.5625\n",
      "Epoch 89, Loss=0.0643, Val Acc=0.5625\n",
      "Epoch 90, Loss=0.0909, Val Acc=0.5625\n",
      "Epoch 91, Loss=0.0945, Val Acc=0.5625\n",
      "Epoch 92, Loss=0.0544, Val Acc=0.5625\n",
      "Epoch 93, Loss=0.0707, Val Acc=0.5625\n",
      "Epoch 94, Loss=0.0681, Val Acc=0.5625\n",
      "Epoch 95, Loss=0.0754, Val Acc=0.5625\n",
      "Epoch 96, Loss=0.0689, Val Acc=0.5625\n",
      "Epoch 97, Loss=0.0582, Val Acc=0.5625\n",
      "Epoch 98, Loss=0.0707, Val Acc=0.5625\n",
      "Epoch 99, Loss=0.0468, Val Acc=0.5625\n",
      "Epoch 100, Loss=0.0760, Val Acc=0.5625\n",
      "Epoch 101, Loss=0.0521, Val Acc=0.5625\n",
      "Epoch 102, Loss=0.0641, Val Acc=0.5625\n",
      "Epoch 103, Loss=0.0673, Val Acc=0.5625\n",
      "Epoch 104, Loss=0.0641, Val Acc=0.5625\n",
      "Epoch 105, Loss=0.0525, Val Acc=0.5625\n",
      "Epoch 106, Loss=0.0450, Val Acc=0.5625\n",
      "Epoch 107, Loss=0.0444, Val Acc=0.5625\n",
      "Epoch 108, Loss=0.0482, Val Acc=0.5625\n",
      "Epoch 109, Loss=0.0422, Val Acc=0.5625\n",
      "Epoch 110, Loss=0.0506, Val Acc=0.5625\n",
      "Epoch 111, Loss=0.0388, Val Acc=0.5625\n",
      "Epoch 112, Loss=0.0435, Val Acc=0.5625\n",
      "Epoch 113, Loss=0.0429, Val Acc=0.5625\n",
      "Epoch 114, Loss=0.0655, Val Acc=0.5625\n",
      "Epoch 115, Loss=0.0320, Val Acc=0.5625\n",
      "Epoch 116, Loss=0.0505, Val Acc=0.5625\n",
      "Epoch 117, Loss=0.0600, Val Acc=0.5625\n",
      "Epoch 118, Loss=0.0536, Val Acc=0.5625\n",
      "Epoch 119, Loss=0.0462, Val Acc=0.5625\n",
      "Epoch 120, Loss=0.0362, Val Acc=0.5625\n",
      "Epoch 121, Loss=0.0336, Val Acc=0.5625\n",
      "Epoch 122, Loss=0.0427, Val Acc=0.5625\n",
      "Epoch 123, Loss=0.0441, Val Acc=0.5625\n",
      "Epoch 124, Loss=0.0342, Val Acc=0.5625\n",
      "Epoch 125, Loss=0.0440, Val Acc=0.5625\n",
      "Epoch 126, Loss=0.0312, Val Acc=0.5625\n",
      "Epoch 127, Loss=0.0400, Val Acc=0.5625\n",
      "Epoch 128, Loss=0.0265, Val Acc=0.5625\n",
      "Epoch 129, Loss=0.0382, Val Acc=0.5625\n",
      "Epoch 130, Loss=0.0306, Val Acc=0.5625\n",
      "Epoch 131, Loss=0.0361, Val Acc=0.5625\n",
      "Epoch 132, Loss=0.0253, Val Acc=0.5625\n",
      "Epoch 133, Loss=0.0415, Val Acc=0.5625\n",
      "Epoch 134, Loss=0.0272, Val Acc=0.5625\n",
      "Epoch 135, Loss=0.0400, Val Acc=0.5625\n",
      "Epoch 136, Loss=0.0302, Val Acc=0.5625\n",
      "Epoch 137, Loss=0.0284, Val Acc=0.5625\n",
      "Epoch 138, Loss=0.0289, Val Acc=0.5625\n",
      "Epoch 139, Loss=0.0302, Val Acc=0.5625\n",
      "Epoch 140, Loss=0.0315, Val Acc=0.5625\n",
      "Epoch 141, Loss=0.0291, Val Acc=0.5625\n",
      "Epoch 142, Loss=0.0242, Val Acc=0.5625\n",
      "Epoch 143, Loss=0.0223, Val Acc=0.5625\n",
      "Epoch 144, Loss=0.0302, Val Acc=0.5625\n",
      "Epoch 145, Loss=0.0322, Val Acc=0.5625\n",
      "Epoch 146, Loss=0.0271, Val Acc=0.5625\n",
      "Epoch 147, Loss=0.0273, Val Acc=0.5625\n",
      "Epoch 148, Loss=0.0307, Val Acc=0.5625\n",
      "Epoch 149, Loss=0.0286, Val Acc=0.5625\n",
      "Epoch 150, Loss=0.0202, Val Acc=0.5625\n",
      "Epoch 151, Loss=0.0188, Val Acc=0.5625\n",
      "Epoch 152, Loss=0.0238, Val Acc=0.5625\n",
      "Epoch 153, Loss=0.0207, Val Acc=0.5625\n",
      "Epoch 154, Loss=0.0242, Val Acc=0.5625\n",
      "Epoch 155, Loss=0.0261, Val Acc=0.5625\n",
      "Epoch 156, Loss=0.0216, Val Acc=0.5625\n",
      "Epoch 157, Loss=0.0184, Val Acc=0.5625\n",
      "Epoch 158, Loss=0.0184, Val Acc=0.5625\n",
      "Epoch 159, Loss=0.0196, Val Acc=0.5625\n",
      "Epoch 160, Loss=0.0175, Val Acc=0.5625\n",
      "Epoch 161, Loss=0.0310, Val Acc=0.5625\n",
      "Epoch 162, Loss=0.0277, Val Acc=0.5625\n",
      "Epoch 163, Loss=0.0173, Val Acc=0.5625\n",
      "Epoch 164, Loss=0.0188, Val Acc=0.5625\n",
      "Epoch 165, Loss=0.0166, Val Acc=0.5625\n",
      "Epoch 166, Loss=0.0249, Val Acc=0.5625\n",
      "Epoch 167, Loss=0.0190, Val Acc=0.5625\n",
      "Epoch 168, Loss=0.0133, Val Acc=0.5625\n",
      "Epoch 169, Loss=0.0240, Val Acc=0.5625\n",
      "Epoch 170, Loss=0.0189, Val Acc=0.5625\n",
      "Epoch 171, Loss=0.0165, Val Acc=0.5625\n",
      "Epoch 172, Loss=0.0220, Val Acc=0.5625\n",
      "Epoch 173, Loss=0.0224, Val Acc=0.5625\n",
      "Epoch 174, Loss=0.0178, Val Acc=0.5625\n",
      "Epoch 175, Loss=0.0212, Val Acc=0.5625\n",
      "Epoch 176, Loss=0.0131, Val Acc=0.5625\n",
      "Epoch 177, Loss=0.0158, Val Acc=0.5625\n",
      "Epoch 178, Loss=0.0191, Val Acc=0.5625\n",
      "Epoch 179, Loss=0.0213, Val Acc=0.5625\n",
      "Epoch 180, Loss=0.0154, Val Acc=0.5625\n",
      "Epoch 181, Loss=0.0172, Val Acc=0.5625\n",
      "Epoch 182, Loss=0.0127, Val Acc=0.5625\n",
      "Epoch 183, Loss=0.0152, Val Acc=0.5625\n",
      "Epoch 184, Loss=0.0151, Val Acc=0.5625\n",
      "Epoch 185, Loss=0.0191, Val Acc=0.5625\n",
      "Epoch 186, Loss=0.0135, Val Acc=0.5625\n",
      "Epoch 187, Loss=0.0161, Val Acc=0.5625\n",
      "Epoch 188, Loss=0.0134, Val Acc=0.5625\n",
      "Epoch 189, Loss=0.0154, Val Acc=0.5625\n",
      "Epoch 190, Loss=0.0115, Val Acc=0.5625\n",
      "Epoch 191, Loss=0.0177, Val Acc=0.5625\n",
      "Epoch 192, Loss=0.0213, Val Acc=0.5625\n",
      "Epoch 193, Loss=0.0147, Val Acc=0.5625\n",
      "Epoch 194, Loss=0.0209, Val Acc=0.5625\n",
      "Epoch 195, Loss=0.0117, Val Acc=0.5625\n",
      "Epoch 196, Loss=0.0158, Val Acc=0.5625\n",
      "Epoch 197, Loss=0.0137, Val Acc=0.5625\n",
      "Epoch 198, Loss=0.0135, Val Acc=0.5625\n",
      "Epoch 199, Loss=0.0166, Val Acc=0.5625\n",
      "Epoch 200, Loss=0.0114, Val Acc=0.5625\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   extrinsic       0.43      0.46      0.44        13\n",
      "   intrinsic       0.38      0.38      0.38        13\n",
      "          no       0.46      0.43      0.44        14\n",
      "\n",
      "    accuracy                           0.42        40\n",
      "   macro avg       0.42      0.42      0.42        40\n",
      "weighted avg       0.43      0.42      0.42        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# BƯỚC 4. Train Classifier (Local)\n",
    "# ============================\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# Load dữ liệu embedding\n",
    "# ----------------------------\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "X_val   = np.load(\"X_val.npy\")\n",
    "X_test  = np.load(\"X_test.npy\")\n",
    "y_train = np.load(\"y_train.npy\", allow_pickle=True)\n",
    "y_val   = np.load(\"y_val.npy\", allow_pickle=True)\n",
    "y_test  = np.load(\"y_test.npy\", allow_pickle=True)\n",
    "\n",
    "# Encode label từ string -> số\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val   = le.transform(y_val)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "# ----------------------------\n",
    "# Chuyển sang Tensor\n",
    "# ----------------------------\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor   = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Dataset + DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset   = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset  = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=16)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# ----------------------------\n",
    "# MLP Classifier\n",
    "# ----------------------------\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Khởi tạo model + optimizer\n",
    "# ----------------------------\n",
    "input_dim = X_train.shape[1]   # embedding dimension (vd: 384)\n",
    "hidden_dim = 128\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "model = MLPClassifier(input_dim, hidden_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Auto chọn device: GPU nếu có, không thì CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training on:\", device)\n",
    "model.to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop\n",
    "# ----------------------------\n",
    "for epoch in range(200):  # chạy 20 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss={total_loss:.4f}, Val Acc={correct/total:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
